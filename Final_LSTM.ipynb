{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09yEwvYJFyW_"
      },
      "source": [
        "# **Data Prep**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drk9cTygIj1-"
      },
      "outputs": [],
      "source": [
        "# Import libraries here\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import MeanAbsolutePercentageError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv('/Users/payalpatel/Downloads/HomeC2.csv', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check time rows to see the data\n",
        "print(df['time'].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check for invalid rows in time (such as //)\n",
        "rows = df[pd.to_numeric(df['time'], errors='coerce').isna()]\n",
        "\n",
        "# remove invalid, keeping only numeric values\n",
        "df = df[pd.to_numeric(df['time'], errors='coerce').notna()] \n",
        "\n",
        "# convert to datetime\n",
        "df['time'] = pd.to_datetime(df['time'], unit='s')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2fueWkrFJ7M"
      },
      "outputs": [],
      "source": [
        "# create variables for weather and energy consumption\n",
        "weather_variables = ['temperature', 'humidity', 'apparentTemperature', 'windSpeed', 'windBearing','dewPoint']\n",
        "target_variable = 'House overall [kW]'\n",
        "\n",
        "dataset = df[[target_variable] + weather_variables].copy()\n",
        "\n",
        "# add rolling and lagged features for dependencies and trends\n",
        "dataset['mean_roll'] = dataset[target_variable].rolling(window=7).mean()\n",
        "dataset['std_roll'] = dataset[target_variable].rolling(window=7).std()\n",
        "\n",
        "dataset['lag'] = dataset[target_variable].shift(1)\n",
        "dataset['2lag'] = dataset[target_variable].shift(2)\n",
        "\n",
        "# fill missing values, if any\n",
        "data = dataset.bfill()\n",
        "\n",
        "# scale target and weather variables separately for better model accuracy \n",
        "scaler = MinMaxScaler()\n",
        "scaled_weather = scaler.fit_transform(data[weather_variables])\n",
        "scaler_target = MinMaxScaler()\n",
        "scaled_target = scaler_target.fit_transform(data[[target_variable]])\n",
        "\n",
        "# create sequences of data for time series prediciton\n",
        "def create_sequences(data, time_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[i:i+time_steps])  \n",
        "        y.append(data[i+time_steps, 0])  \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# combine weather and energy consumption variables for sequence\n",
        "combine_t_w = np.hstack([scaled_weather, scaled_target])\n",
        "\n",
        "# predictions are for 7 days (24 (hours) * 7 (days)= 168)\n",
        "time_steps = 168 \n",
        "\n",
        "# create sequences\n",
        "X, y = create_sequences(combine_t_w, time_steps)\n",
        "\n",
        "# 80% training and 20% testing split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# y_test has one NaN, we can replace it with the mean value\n",
        "y_test = np.nan_to_num(y_test, nan=np.nanmean(y_test)) \n",
        "\n",
        "X_train = np.nan_to_num(X_train)\n",
        "y_train = np.nan_to_num(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id3yqWfnIoeI"
      },
      "source": [
        "# **LSTM Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cml4Hdj4Ips9"
      },
      "outputs": [],
      "source": [
        "# define the LSTM model - add input, output, dense, LSTM \n",
        "model = Sequential()\n",
        "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=100, return_sequences=True))\n",
        "model.add(LSTM(units=50, return_sequences=False))\n",
        "model.add(Dense(units=25, activation='relu'))\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# use adam optimization\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', MeanAbsolutePercentageError()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHkhN6x7Iqf_"
      },
      "source": [
        "# **Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzjqrzpvIwbu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m  57/3149\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:16\u001b[0m 199ms/step - loss: 0.0466 - mae: 0.1427 - mean_absolute_percentage_error: 26.4527"
          ]
        }
      ],
      "source": [
        "# early stop (7 epochs before stopping)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "\n",
        "# Train LSTM\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oOgVGiIxSp"
      },
      "source": [
        "# **Evaluate the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLmjp4-9I06s"
      },
      "outputs": [],
      "source": [
        "# evaluate LSTM model using mae (but subtract by 100)\n",
        "loss, mae, mape = model.evaluate(X_test, y_test, verbose=0)\n",
        "accuracy = 100 - mape\n",
        "print(f\"Accuracy: {accuracy:2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDYoZo31I9qI"
      },
      "source": [
        "# **Future Energy Consumption**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# same as time_steps 168 hours of predictions\n",
        "predictions = 7 * 24\n",
        "\n",
        "# use the last datapoint to start future predictions\n",
        "last_point = X_test[-1].reshape(1, time_steps, X_test.shape[2])\n",
        "\n",
        "# store the prediction \n",
        "future_prediction = []\n",
        "\n",
        "# generate the next prediction\n",
        "for _ in range(predictions):\n",
        "    next = model.predict(last_point, verbose=0)  \n",
        "    future_prediction.append(next[0, 0])  \n",
        "\n",
        "    # update the last point and add next prediction\n",
        "    last_point = np.roll(last_point, shift=-1, axis=1)\n",
        "    last_point[0, -1, -1] = next  \n",
        "\n",
        "# make sure predictions are back to the orignal scale (use reshape)\n",
        "original_pred = scaler_target.inverse_transform(np.array(future_prediction).reshape(-1, 1))\n",
        "\n",
        "# aggregate predictions into daily average\n",
        "daily_avg = np.mean(original_pred.reshape(7, 24), axis=1)\n",
        "\n",
        "# since we are plots 7 days, create labels for each of the 7 days\n",
        "label_day = [f\"Day {i+1}\" for i in range(7)]\n",
        "\n",
        "# plot predictions for 7 days (add points for each day)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(label_day, daily_avg, marker='o', color='blue', label='Predicted Energy Consumption')\n",
        "plt.xlabel('Next 7 Days')\n",
        "plt.ylabel('Energy Consumption (kW)')\n",
        "plt.title('Energy Consumption Forecast - Next 7 Days ')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='skyblue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert actual test values back to original scale\n",
        "y_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# plot actual and predicted \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_rescaled, label='Actual', color='skyblue')\n",
        "plt.plot(future_prediction, label='Predicted', color='orange')\n",
        "plt.xlabel('Time (Hour)')\n",
        "plt.ylabel('Energy Consumption (kW)')\n",
        "plt.title('Actual vs Predicted Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataframe for actual and predicted daily values\n",
        "data = {\n",
        "    'Day': label_day,\n",
        "    'Actual Energy Consumption (kW)': y_rescaled,\n",
        "    'Predicted Energy Consumption (kW)': daily_avg\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# save csv file\n",
        "df.to_csv('/Users/payalpatel/Downloads/energy_consumption_forecast.csv', index=False)\n",
        "\n",
        "print(\"Saved\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
